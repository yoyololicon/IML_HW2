# Homework 2 for Introduction to Machine Learning

## 開發環境和語言

詳細資訊可查看[readme](README.md)。

## K-NN Classifier

### 原理

實做K-D tree並不困難，只要不斷的找資料的中位數，分割後往下遞迴就可。但是要找K-nearest neighbors就有點麻煩。

先從k=1找least nearest neighbor 開始。
當我們從root依照分割條件找到最後的leaf node後，因為leaf node不一定是最近的，還要往上回推查看有沒有其他的數值離測試資料更近的。
當K>1，就需要有一個ordered stack來保存目前找到的前k個最近的instance。

而我做了一個叫KNN hyperplane的function來找nearest neighbors。以下為其虛擬碼。

```
KNN_hyperplane(root, value, k):
    parents = empty
    eliminate = empty
    k_stack = empty
    node = KNN_search(root, parents, value)
    
    //表示visited，之後就不用再查看
    add node to eliminate 
    //第一個最近的instance
    add node to k_stack

    while parents:
        //之前去過得node的parents
        p = parents.pop()
        while p in eliminate:
            p = parents.pop()
        //加入visited
        add p to eliminate
        add p to k_stack
        //目前最大半徑
        radius = max_euclidian_distance(k_stack)
        
        //可能有沒去過得node包含在目前的最大半徑距離內
        if abs(value[axis] - p.[axis]) <= radius:
            if value[axis] >= p[axis] and p.left_child not in eliminate:
                //加入visited
                node = KNN_search(p.left_child, parents, value)
                add p to eliminate
                add p to k_stack
            else if p.right_child not in eliminate:
                node = KNN_search(p.right_child, parents, value)
                //加入visited
                add p to eliminate
                add p to k_stack

    return k_stack
```
### 實驗
由於沒有test data，所以我從train data 隨機挑了36個資料來做evaluation。以下為結果：
```
KNN accuracy:  1.0
286
151
149

KNN accuracy:  0.861111111111
286 87 252 142 226
151 282 276 71 221
149 219 257 283 79

KNN accuracy:  0.888888888889
286 87 252 142 226 158 60 247 141 293
151 282 276 71 221 84 81 289 49 10
149 219 257 283 79 239 249 227 210 99

KNN accuracy:  0.583333333333
286 87 252 142 226 158 60 247 141 293 85 171 202 299 30 111 150 44 3 223 198 102 263 193 96 209 143 119 145 15 163 39 211 180 68 29 284 120 53 237 0 205 113 80 50 98 296 267 196 212 40 23 241 118 190 16 189 240 56 115 191 259 106 156 235 4 246 162 101 2 70 233 238 109 173 184 270 36 54 243 37 279 116 177 41 108 260 297 27 242 187 206 232 281 124 217 200 225 288 265
151 282 276 71 221 84 81 289 49 10 195 92 117 174 91 9 199 274 79 253 82 227 292 34 249 69 140 250 181 191 201 53 271 260 72 210 239 216 288 265 219 284 48 76 184 215 175 121 161 232 143 194 208 104 133 62 243 182 285 12 183 228 146 149 19 261 267 254 125 266 189 236 124 14 220 205 89 272 57 65 100 108 235 264 294 96 162 46 127 196 230 296 238 45 80 200 283 66 139 99
149 219 257 283 79 239 249 227 210 99 67 285 34 91 169 215 33 264 97 155 251 78 112 51 253 195 18 21 43 65 290 49 127 133 170 100 199 140 216 66 82 292 161 224 276 288 262 207 234 274 174 229 52 117 282 186 166 184 88 135 110 74 261 7 77 9 182 179 46 72 295 176 238 139 56 194 157 289 153 231 57 10 12 269 152 201 19 258 20 105 244 28 221 172 93 92 81 275 151 71
```
我自己有再寫一個使用sklearn的KNN當作對照組，確信兩邊的結果都相同。

從結果來看k=1的accuracy可以到100%並不意外，因為test data包含在train data裡面，其結果不能當作參考。
所以k=10的時候效果最好。k=100時可能就開始包含到別的類別資料數量變多，就變得不準。

## KNN with PCA

### 原理

依照定義，PCA是找出輸入資料covariance的特徵向量來做轉換、分析，可以只取前幾個較重要的特徵向量來分析，減少輸入維度。而我將這些資料再丟入KD tree做分類，檢測其效能是否會更好。

### 實驗
使用了相同train/test data、測試不同數量的特徵向量後，發現使用前六個特徵向量轉換後的六維資料，可以比原本的九維資料，達到更高的平均accuracy。

只使用六維資料的結果：
```
 " 6 Dimension"
KNN accuracy:  1.0
286
151
149

KNN accuracy:  0.861111111111
286 87 252 142 226
151 276 282 71 221
149 219 283 257 79

KNN accuracy:  0.888888888889
286 87 252 142 226 158 247 60 293 85
151 276 282 71 221 81 84 289 45 49
149 219 283 257 79 239 262 227 215 249

KNN accuracy:  0.666666666667
286 87 252 142 226 158 247 60 293 85 141 171 202 30 299 111 150 44 3 223 198 102 263 193 96 209 143 119 145 15 163 39 180 211 68 29 284 120 53 237 0 113 205 80 50 267 98 296 196 40 129 212 23 241 118 190 16 240 189 56 191 115 106 259 235 4 156 246 70 162 101 2 233 238 109 173 184 243 36 270 197 248 54 37 279 177 116 108 297 41 260 27 242 232 187 206 124 281 217 225
151 276 282 71 221 81 84 289 45 49 10 195 92 175 117 104 174 91 9 82 274 199 79 249 227 253 292 140 69 34 89 191 201 250 53 181 210 260 271 72 239 288 216 76 6 219 265 184 284 48 232 121 161 215 143 208 194 149 133 62 267 285 294 19 14 12 183 243 228 182 146 261 254 125 266 236 272 124 220 189 57 235 205 108 65 264 100 162 96 127 196 46 230 238 296 66 99 80 139 70
149 219 283 257 79 239 262 227 215 249 91 210 99 34 67 285 33 264 169 155 97 224 253 251 78 89 112 195 51 65 18 43 290 49 21 127 199 100 133 170 216 292 174 140 66 82 161 117 288 276 274 207 234 52 229 282 55 186 104 295 221 166 110 88 135 184 261 46 7 182 74 9 72 179 176 77 238 194 6 139 56 157 10 231 151 153 250 57 289 12 244 275 258 20 269 105 201 152 19 172
```
發現在k=100時的accuracy會比原本高，可以當作是經過PCA分析後的資料，在空間分部的相對位置，比原資料準確的證明。

而只使用前兩個特徵向量時，轉換後的資料分佈長這樣：
![](pca_2d.png)

在中間有一道明顯的分界，分隔出im, imU（左側）和其他的類別（右側），且這兩個特徵向量成反比關係。

在最後版本的code裡的PCA會自動判斷哪種維度會有最高的平均accuracy，然後輸出該維度k=5的結果。